{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CS159_final.ipynb copy",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I016jA0vv2ue",
        "outputId": "2f524808-9a55-47ff-801d-c36f354a9e97"
      },
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "from gym.spaces import Discrete, Box\n",
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import random\n",
        "import gym\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nDOTqNL3wlOm"
      },
      "source": [
        "class DeterministicDiscreteActionLinearPolicy(object):\n",
        "\n",
        "    def __init__(self, theta, ob_space, ac_space):\n",
        "        \"\"\"\n",
        "        dim_ob: dimension of observations\n",
        "        n_actions: number of actions\n",
        "        theta: flat vector of parameters\n",
        "        \"\"\"\n",
        "        dim_ob = ob_space.shape[0]\n",
        "        n_actions = ac_space.n\n",
        "        assert len(theta) == (dim_ob + 1) * n_actions\n",
        "        self.W = theta[0 : dim_ob * n_actions].reshape(dim_ob, n_actions)\n",
        "        self.b = theta[dim_ob * n_actions : None].reshape(1, n_actions)\n",
        "\n",
        "    def act(self, ob):\n",
        "        \"\"\"\n",
        "        \"\"\"\n",
        "        y = ob.dot(self.W) + self.b\n",
        "        a = y.argmax()\n",
        "        return a"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ov5I13I-xKON"
      },
      "source": [
        "def do_episode(policy, env, num_steps, discount=1.0, render=False):\n",
        "    disc_total_rew = 0\n",
        "    ob = env.reset()\n",
        "    for t in range(num_steps):\n",
        "        a = policy.act(ob)\n",
        "        (ob, reward, done, _info) = env.step(a)\n",
        "        disc_total_rew += reward * discount**t\n",
        "        if render and t%3==0:\n",
        "            env.render()\n",
        "        if done: break\n",
        "    return disc_total_rew\n",
        "\n",
        "def noisy_evaluation(theta, discount=1.0):\n",
        "    policy = make_policy(theta)\n",
        "    reward = do_episode(policy, env, num_steps, discount)\n",
        "    return reward\n",
        "\n",
        "def make_policy(theta):\n",
        "    return DeterministicDiscreteActionLinearPolicy(theta, env.observation_space, env.action_space)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9kxSO-nsxg4b"
      },
      "source": [
        "# Task settings:\n",
        "env = gym.make('CartPole-v0') # Change as needed\n",
        "num_steps = 200 # maximum length of episode\n",
        "\n",
        "# Alg settings:\n",
        "n_iter = 20 # number of iterations of CEM\n",
        "batch_size = 25 # number of samples per batch\n",
        "elite_frac = 0.2 # fraction of samples used as elite set\n",
        "n_elite = int(batch_size * elite_frac)\n",
        "extra_std = 2.0\n",
        "extra_decay_time = 10\n",
        "dim_theta = (env.observation_space.shape[0]+1) * env.action_space.n\n",
        "\n",
        "# Initialize mean and standard deviation\n",
        "theta_mean = np.zeros(dim_theta)\n",
        "theta_std = np.ones(dim_theta)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NYS9ZuS9xXQ"
      },
      "source": [
        "## Cross Entropy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCP2gqk4DQCu"
      },
      "source": [
        "#### Train the agent using cross entropy policy optimization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KT6k-UFex1uv",
        "outputId": "53f62e45-4220-419a-8d7d-dcd255647e0e"
      },
      "source": [
        "for itr in range(n_iter):\n",
        "    # Sample parameter vectors\n",
        "    extra_cov = max(1.0 - itr / extra_decay_time, 0) * extra_std**2\n",
        "    thetas = np.random.multivariate_normal(mean=theta_mean, \n",
        "                                           cov=np.diag(np.array(theta_std**2) + extra_cov), \n",
        "                                           size=batch_size)\n",
        "    rewards = np.array([noisy_evaluation(theta) for theta in thetas])\n",
        "\n",
        "    # Get elite parameters\n",
        "    elite_inds = rewards.argsort()[-n_elite:]\n",
        "    elite_thetas = thetas[elite_inds]\n",
        "\n",
        "    # Update theta_mean, theta_std\n",
        "    theta_mean = elite_thetas.mean(axis=0)\n",
        "    theta_std = elite_thetas.std(axis=0)\n",
        "    print(\"iteration {} mean {} max {}\".format(itr, np.mean(rewards), np.max(rewards)))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iteration 0 mean 11.08 max 28.0\n",
            "iteration 1 mean 19.2 max 59.0\n",
            "iteration 2 mean 52.48 max 200.0\n",
            "iteration 3 mean 50.16 max 200.0\n",
            "iteration 4 mean 76.2 max 200.0\n",
            "iteration 5 mean 74.88 max 200.0\n",
            "iteration 6 mean 125.48 max 200.0\n",
            "iteration 7 mean 143.4 max 200.0\n",
            "iteration 8 mean 126.44 max 200.0\n",
            "iteration 9 mean 140.44 max 200.0\n",
            "iteration 10 mean 195.08 max 200.0\n",
            "iteration 11 mean 200.0 max 200.0\n",
            "iteration 12 mean 200.0 max 200.0\n",
            "iteration 13 mean 200.0 max 200.0\n",
            "iteration 14 mean 200.0 max 200.0\n",
            "iteration 15 mean 200.0 max 200.0\n",
            "iteration 16 mean 200.0 max 200.0\n",
            "iteration 17 mean 200.0 max 200.0\n",
            "iteration 18 mean 200.0 max 200.0\n",
            "iteration 19 mean 200.0 max 200.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfouon4RDQCv"
      },
      "source": [
        "#### Testing the final cross entropy agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kYJBJwF9DQCv",
        "outputId": "671ed675-641b-4893-c70d-05e0295a8d8b"
      },
      "source": [
        "print(\"reward {}\".format(do_episode(make_policy(theta_mean), env, num_steps, render=False)))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "reward 200.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EBDTatam9sms"
      },
      "source": [
        "## Random Shooting\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDFM1VnFDQCw"
      },
      "source": [
        "#### Training and testing the random shooting agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9G7W-ipC7rZe",
        "outputId": "fb5274fa-ae11-443b-b7fb-4695fbc861ed"
      },
      "source": [
        "t = 0\n",
        "n_iter = 100\n",
        "for i in range(n_iter):\n",
        "    thetas = np.random.multivariate_normal(mean=np.zeros(dim_theta), \n",
        "                                               cov=np.diag(np.ones(dim_theta)), \n",
        "                                               size=batch_size)\n",
        "    rewards = np.array([noisy_evaluation(theta) for theta in thetas])\n",
        "    elite_ind = rewards.argmax()\n",
        "    elite_theta = thetas[elite_ind]\n",
        "    t += do_episode(make_policy(elite_theta), env, num_steps, render=False)\n",
        "print(\"average reward {}\".format(t / n_iter))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "average reward 91.77\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-zWqo2gA5Fq"
      },
      "source": [
        "## Gradient Descent with Policy Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdVJabwn98Mb"
      },
      "source": [
        "def policy_gradient():\n",
        "    with tf.variable_scope(\"policy\", reuse=tf.AUTO_REUSE):\n",
        "        params = tf.get_variable(\"policy_parameters\",[4,2])\n",
        "        state = tf.placeholder(\"float\",[None,4])\n",
        "        actions = tf.placeholder(\"float\",[None,2])\n",
        "        advantages = tf.placeholder(\"float\",[None,1])\n",
        "        linear = tf.matmul(state,params)\n",
        "        probabilities = tf.nn.softmax(linear)\n",
        "        good_probabilities = tf.reduce_sum(tf.multiply(probabilities, actions),reduction_indices=[1])\n",
        "        eligibility = tf.log(good_probabilities) * advantages\n",
        "        loss = -tf.reduce_sum(eligibility)\n",
        "        optimizer = tf.train.AdamOptimizer(0.01).minimize(loss)\n",
        "        return probabilities, state, actions, advantages, optimizer\n",
        "\n",
        "def value_gradient():\n",
        "    with tf.variable_scope(\"value\", reuse=tf.AUTO_REUSE):\n",
        "        state = tf.placeholder(\"float\",[None,4])\n",
        "        newvals = tf.placeholder(\"float\",[None,1])\n",
        "        w1 = tf.get_variable(\"w1\",[4,10])\n",
        "        b1 = tf.get_variable(\"b1\",[10])\n",
        "        h1 = tf.nn.relu(tf.matmul(state,w1) + b1)\n",
        "        w2 = tf.get_variable(\"w2\",[10,1])\n",
        "        b2 = tf.get_variable(\"b2\",[1])\n",
        "        calculated = tf.matmul(h1,w2) + b2\n",
        "        diffs = calculated - newvals\n",
        "        loss = tf.nn.l2_loss(diffs)\n",
        "        optimizer = tf.train.AdamOptimizer(0.1).minimize(loss)\n",
        "        return calculated, state, newvals, optimizer, loss\n",
        "\n",
        "def run_episode(env, policy_grad, value_grad, sess, render=False, iter=None, threshold=None):\n",
        "    pl_calculated, pl_state, pl_actions, pl_advantages, pl_optimizer = policy_grad\n",
        "    vl_calculated, vl_state, vl_newvals, vl_optimizer, vl_loss = value_grad\n",
        "    observation = env.reset()\n",
        "    totalreward = 0\n",
        "    states = []\n",
        "    actions = []\n",
        "    advantages = []\n",
        "    transitions = []\n",
        "    update_vals = []\n",
        "\n",
        "    for t in range(200):\n",
        "        # calculate policy\n",
        "        obs_vector = np.expand_dims(observation, axis=0)\n",
        "        probs = sess.run(pl_calculated,feed_dict={pl_state: obs_vector})\n",
        "        action = 0 if random.uniform(0,1) < probs[0][0] else 1\n",
        "        # record the transition\n",
        "        states.append(observation)\n",
        "        actionblank = np.zeros(2)\n",
        "        actionblank[action] = 1\n",
        "        actions.append(actionblank)\n",
        "        # take the action in the environment\n",
        "        old_observation = observation\n",
        "        observation, reward, done, info = env.step(action)\n",
        "        transitions.append((old_observation, action, reward))\n",
        "        totalreward += reward\n",
        "        \n",
        "        if t % 3 == 0 and render:\n",
        "            env.render()\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "            \n",
        "    if threshold and threshold(totalreward, iter):\n",
        "        return totalreward, 1\n",
        "    \n",
        "    for index, trans in enumerate(transitions):\n",
        "        obs, action, reward = trans\n",
        "\n",
        "        # calculate discounted monte-carlo return\n",
        "        future_reward = 0\n",
        "        future_transitions = len(transitions) - index\n",
        "        decrease = 1\n",
        "        for index2 in range(future_transitions):\n",
        "            future_reward += transitions[(index2) + index][2] * decrease\n",
        "            decrease = decrease * 0.97\n",
        "        obs_vector = np.expand_dims(obs, axis=0)\n",
        "        currentval = sess.run(vl_calculated,feed_dict={vl_state: obs_vector})[0][0]\n",
        "\n",
        "        # advantage: how much better was this action than normal\n",
        "        advantages.append(future_reward - currentval)\n",
        "\n",
        "        # update the value function towards new return\n",
        "        update_vals.append(future_reward)\n",
        "\n",
        "    # update value function\n",
        "    update_vals_vector = np.expand_dims(update_vals, axis=1)\n",
        "    sess.run(vl_optimizer, feed_dict={vl_state: states, vl_newvals: update_vals_vector})\n",
        "\n",
        "    advantages_vector = np.expand_dims(advantages, axis=1)\n",
        "    sess.run(pl_optimizer, feed_dict={pl_state: states, pl_advantages: advantages_vector, pl_actions: actions})\n",
        "\n",
        "    return totalreward, 0"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28esuh6UDQCy"
      },
      "source": [
        "#### Training the policy network agent using gradient descent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_1qXVN0ACf2Z",
        "outputId": "51a44153-c802-418a-9060-7b1af41c38e8"
      },
      "source": [
        "env = gym.make('CartPole-v0')\n",
        "policy_grad = policy_gradient()\n",
        "value_grad = value_gradient()\n",
        "sess = tf.InteractiveSession()\n",
        "sess.run(tf.initialize_all_variables())\n",
        "t = 0\n",
        "for i in range(2000):\n",
        "    reward, _ = run_episode(env, policy_grad, value_grad, sess)\n",
        "    t += reward\n",
        "    if i % 100 == 0:\n",
        "        print(\"iteration {} mean {}\".format(i, t / (i + 1)))\n",
        "print(\"final mean {}\".format(t / 2000))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/util/tf_should_use.py:198: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
            "Instructions for updating:\n",
            "Use `tf.global_variables_initializer` instead.\n",
            "iteration 0 mean 10.0\n",
            "iteration 100 mean 22.237623762376238\n",
            "iteration 200 mean 28.0\n",
            "iteration 300 mean 44.77076411960133\n",
            "iteration 400 mean 67.71820448877806\n",
            "iteration 500 mean 84.24550898203593\n",
            "iteration 600 mean 99.50415973377704\n",
            "iteration 700 mean 110.02853067047076\n",
            "iteration 800 mean 118.3632958801498\n",
            "iteration 900 mean 123.80910099889012\n",
            "iteration 1000 mean 129.44455544455545\n",
            "iteration 1100 mean 134.5068119891008\n",
            "iteration 1200 mean 138.4338051623647\n",
            "iteration 1300 mean 142.06764027671022\n",
            "iteration 1400 mean 144.1163454675232\n",
            "iteration 1500 mean 145.58760826115923\n",
            "iteration 1600 mean 146.99625234228608\n",
            "iteration 1700 mean 149.16519694297472\n",
            "iteration 1800 mean 150.91504719600223\n",
            "iteration 1900 mean 152.74960547080485\n",
            "final mean 154.406\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2HNZwTX4DQCz"
      },
      "source": [
        "#### Testing the policy network agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "042mMVmcDQCz",
        "outputId": "f9e69f82-f2bf-4027-d624-0d83c75ce665"
      },
      "source": [
        "t = 0\n",
        "for i in range(100):\n",
        "  reward, _ = run_episode(env, policy_grad, value_grad, sess)\n",
        "  t += reward\n",
        "print(\"test reward {}\".format(t / 100))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test reward 181.57\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MewwBEC5DQCz"
      },
      "source": [
        "## Policy Network with Cross Entropy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12G4GMNVDQCz"
      },
      "source": [
        "#### Training the policy network agent with cross entropy using gradient descent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EBfwFVFCDQCz",
        "outputId": "23049b94-9902-4c6e-fc97-dfd55668b792"
      },
      "source": [
        "def threshold_check(totalreward, iter):    \n",
        "    return totalreward < min(200, iter * 0.1)\n",
        "\n",
        "env = gym.make('CartPole-v0')\n",
        "policy_grad = policy_gradient()\n",
        "value_grad = value_gradient()\n",
        "sess = tf.InteractiveSession()\n",
        "sess.run(tf.initialize_all_variables())\n",
        "t = 0\n",
        "skipped = 0\n",
        "for i in range(2000):\n",
        "    reward, skip = run_episode(env, policy_grad, value_grad, sess, render=False, iter=i, threshold=threshold_check)\n",
        "    t += reward\n",
        "    skipped += skip\n",
        "    if i % 100 == 0:\n",
        "        print(\"iteration {} mean {} skipped {}\".format(i, t / (i + 1), skipped))\n",
        "print(\"final mean {}\".format(t / 2000))\n",
        "print(\"iterations skipped {}\".format(skipped))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/tensorflow-1.15.2/python3.7/tensorflow_core/python/client/session.py:1750: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
            "  warnings.warn('An interactive session is already active. This can '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "iteration 0 mean 42.0 skipped 0\n",
            "iteration 100 mean 24.673267326732674 skipped 0\n",
            "iteration 200 mean 31.407960199004975 skipped 10\n",
            "iteration 300 mean 36.56810631229236 skipped 33\n",
            "iteration 400 mean 46.119700748129674 skipped 53\n",
            "iteration 500 mean 60.08582834331337 skipped 66\n",
            "iteration 600 mean 74.89351081530782 skipped 78\n",
            "iteration 700 mean 88.15549215406563 skipped 84\n",
            "iteration 800 mean 95.83770287141074 skipped 95\n",
            "iteration 900 mean 103.0033296337403 skipped 103\n",
            "iteration 1000 mean 110.86413586413586 skipped 111\n",
            "iteration 1100 mean 117.79019073569482 skipped 114\n",
            "iteration 1200 mean 122.81931723563697 skipped 119\n",
            "iteration 1300 mean 125.82244427363567 skipped 133\n",
            "iteration 1400 mean 128.8372591006424 skipped 154\n",
            "iteration 1500 mean 131.76882078614258 skipped 175\n",
            "iteration 1600 mean 134.0624609618988 skipped 202\n",
            "iteration 1700 mean 135.885949441505 skipped 236\n",
            "iteration 1800 mean 137.25763464741812 skipped 277\n",
            "iteration 1900 mean 138.4902682798527 skipped 322\n",
            "final mean 139.8055\n",
            "iterations skipped 366\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4r8qDfjzDQC0"
      },
      "source": [
        "#### Testing the cross entropy policy network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VjDBNbNfDQC0",
        "outputId": "dd9ebe12-70b1-40f1-fa41-a94dad192502"
      },
      "source": [
        "t = 0\n",
        "for i in range(100):\n",
        "  reward, _ = run_episode(env, policy_grad, value_grad, sess)\n",
        "  t += reward\n",
        "print(\"test reward {}\".format(t / 100))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test reward 168.81\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RmEsW5rRJF6t"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}