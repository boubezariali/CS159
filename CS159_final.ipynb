{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I016jA0vv2ue",
    "outputId": "3618c854-caf8-4df3-f83c-12462d953b06"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from gym.spaces import Discrete, Box\n",
    "# %tensorflow_version 1.x\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import gym\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "nDOTqNL3wlOm"
   },
   "outputs": [],
   "source": [
    "class DeterministicDiscreteActionLinearPolicy(object):\n",
    "\n",
    "    def __init__(self, theta, ob_space, ac_space):\n",
    "        \"\"\"\n",
    "        dim_ob: dimension of observations\n",
    "        n_actions: number of actions\n",
    "        theta: flat vector of parameters\n",
    "        \"\"\"\n",
    "        dim_ob = ob_space.shape[0]\n",
    "        n_actions = ac_space.n\n",
    "        assert len(theta) == (dim_ob + 1) * n_actions\n",
    "        self.W = theta[0 : dim_ob * n_actions].reshape(dim_ob, n_actions)\n",
    "        self.b = theta[dim_ob * n_actions : None].reshape(1, n_actions)\n",
    "\n",
    "    def act(self, ob):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        y = ob.dot(self.W) + self.b\n",
    "        a = y.argmax()\n",
    "        return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ov5I13I-xKON"
   },
   "outputs": [],
   "source": [
    "def do_episode(policy, env, num_steps, discount=1.0, render=False):\n",
    "    disc_total_rew = 0\n",
    "    ob = env.reset()\n",
    "    for t in range(num_steps):\n",
    "        a = policy.act(ob)\n",
    "        (ob, reward, done, _info) = env.step(a)\n",
    "        disc_total_rew += reward * discount**t\n",
    "        if render and t%3==0:\n",
    "            env.render()\n",
    "        if done: break\n",
    "    return disc_total_rew\n",
    "\n",
    "env = None\n",
    "def noisy_evaluation(theta, discount=1.0):\n",
    "    policy = make_policy(theta)\n",
    "    reward = do_episode(policy, env, num_steps, discount)\n",
    "    return reward\n",
    "\n",
    "def make_policy(theta):\n",
    "    return DeterministicDiscreteActionLinearPolicy(theta, env.observation_space, env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "9kxSO-nsxg4b"
   },
   "outputs": [],
   "source": [
    "# Task settings:\n",
    "env = gym.make('CartPole-v0') # Change as needed\n",
    "num_steps = 200 # maximum length of episode\n",
    "# env.monitor.start('/tmp/cartpole-experiment-1')\n",
    "\n",
    "# Alg settings:\n",
    "n_iter = 20 # number of iterations of CEM\n",
    "batch_size = 25 # number of samples per batch\n",
    "elite_frac = 0.2 # fraction of samples used as elite set\n",
    "n_elite = int(batch_size * elite_frac)\n",
    "extra_std = 2.0\n",
    "extra_decay_time = 10\n",
    "dim_theta = (env.observation_space.shape[0]+1) * env.action_space.n\n",
    "\n",
    "# Initialize mean and standard deviation\n",
    "theta_mean = np.zeros(dim_theta)\n",
    "theta_std = np.ones(dim_theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-NYS9ZuS9xXQ"
   },
   "source": [
    "## CEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KT6k-UFex1uv",
    "outputId": "0c30019b-7f12-4d8c-fbe1-164b34c7414a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 mean 16.6 max 114.0\n",
      "iteration 1 mean 21.28 max 81.0\n",
      "iteration 2 mean 36.48 max 129.0\n",
      "iteration 3 mean 54.4 max 138.0\n",
      "iteration 4 mean 79.16 max 200.0\n",
      "iteration 5 mean 67.68 max 200.0\n",
      "iteration 6 mean 115.68 max 200.0\n",
      "iteration 7 mean 150.48 max 200.0\n",
      "iteration 8 mean 191.8 max 200.0\n",
      "iteration 9 mean 184.28 max 200.0\n",
      "iteration 10 mean 184.32 max 200.0\n",
      "iteration 11 mean 162.0 max 200.0\n",
      "iteration 12 mean 200.0 max 200.0\n",
      "iteration 13 mean 199.52 max 200.0\n",
      "iteration 14 mean 200.0 max 200.0\n",
      "iteration 15 mean 200.0 max 200.0\n",
      "iteration 16 mean 200.0 max 200.0\n",
      "iteration 17 mean 200.0 max 200.0\n",
      "iteration 18 mean 200.0 max 200.0\n",
      "iteration 19 mean 200.0 max 200.0\n"
     ]
    }
   ],
   "source": [
    "# Now, for the algorithm\n",
    "for itr in range(n_iter):\n",
    "    # Sample parameter vectors\n",
    "    extra_cov = max(1.0 - itr / extra_decay_time, 0) * extra_std**2\n",
    "    thetas = np.random.multivariate_normal(mean=theta_mean, \n",
    "                                           cov=np.diag(np.array(theta_std**2) + extra_cov), \n",
    "                                           size=batch_size)\n",
    "    #rewards = np.array(map(noisy_evaluation, thetas))\n",
    "    rewards = np.array([noisy_evaluation(theta) for theta in thetas])\n",
    "\n",
    "    # Get elite parameters\n",
    "    elite_inds = rewards.argsort()[-n_elite:]\n",
    "    elite_thetas = thetas[elite_inds]\n",
    "\n",
    "    # Update theta_mean, theta_std\n",
    "    theta_mean = elite_thetas.mean(axis=0)\n",
    "    theta_std = elite_thetas.std(axis=0)\n",
    "    print(\"iteration {} mean {} max {}\".format(itr, np.mean(rewards), np.max(rewards)))\n",
    "    do_episode(make_policy(theta_mean), env, num_steps, discount=0.90, render=True)\n",
    "\n",
    "#     print(\"theta mean\", theta_mean)\n",
    "#     print(\"theta std\", theta_std)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EBDTatam9sms"
   },
   "source": [
    "## Random Shooting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9G7W-ipC7rZe",
    "outputId": "5f26a65c-ced3-4d3f-f6d5-c50a471b0415"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best theta: [ 0.13538736  0.13866565  0.00339023 -1.1278153  -1.27884826 -0.73940764\n",
      "  0.38156817  0.80112322  0.52504811  0.4836304 ]\n",
      "reward: 69.0\n"
     ]
    }
   ],
   "source": [
    "thetas = np.random.uniform(size = batch_size)\n",
    "thetas = np.random.multivariate_normal(mean=np.zeros(dim_theta), \n",
    "                                           cov=np.diag(np.ones(dim_theta)), \n",
    "                                           size=batch_size)\n",
    "rewards = np.array([noisy_evaluation(theta) for theta in thetas])\n",
    "elite_ind = rewards.argmax()\n",
    "elite_theta = thetas[elite_ind]\n",
    "\n",
    "print(\"best theta:\", elite_theta)\n",
    "print(\"reward:\", rewards[elite_ind])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L-zWqo2gA5Fq"
   },
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "bdVJabwn98Mb"
   },
   "outputs": [],
   "source": [
    "def policy_gradient():\n",
    "    with tf.variable_scope(\"policy\", reuse=tf.AUTO_REUSE):\n",
    "        params = tf.get_variable(\"policy_parameters\",[4,2])\n",
    "        state = tf.placeholder(\"float\",[None,4])\n",
    "        actions = tf.placeholder(\"float\",[None,2])\n",
    "        advantages = tf.placeholder(\"float\",[None,1])\n",
    "        linear = tf.matmul(state,params)\n",
    "        probabilities = tf.nn.softmax(linear)\n",
    "        good_probabilities = tf.reduce_sum(tf.multiply(probabilities, actions),reduction_indices=[1])\n",
    "        eligibility = tf.log(good_probabilities) * advantages\n",
    "        loss = -tf.reduce_sum(eligibility)\n",
    "        optimizer = tf.train.AdamOptimizer(0.01).minimize(loss)\n",
    "        return probabilities, state, actions, advantages, optimizer\n",
    "\n",
    "def value_gradient():\n",
    "    with tf.variable_scope(\"value\", reuse=tf.AUTO_REUSE):\n",
    "        state = tf.placeholder(\"float\",[None,4])\n",
    "        newvals = tf.placeholder(\"float\",[None,1])\n",
    "        w1 = tf.get_variable(\"w1\",[4,10])\n",
    "        b1 = tf.get_variable(\"b1\",[10])\n",
    "        h1 = tf.nn.relu(tf.matmul(state,w1) + b1)\n",
    "        w2 = tf.get_variable(\"w2\",[10,1])\n",
    "        b2 = tf.get_variable(\"b2\",[1])\n",
    "        calculated = tf.matmul(h1,w2) + b2\n",
    "        diffs = calculated - newvals\n",
    "        loss = tf.nn.l2_loss(diffs)\n",
    "        optimizer = tf.train.AdamOptimizer(0.1).minimize(loss)\n",
    "        return calculated, state, newvals, optimizer, loss\n",
    "\n",
    "def run_episode(env, policy_grad, value_grad, sess, render=False):\n",
    "    pl_calculated, pl_state, pl_actions, pl_advantages, pl_optimizer = policy_grad\n",
    "    vl_calculated, vl_state, vl_newvals, vl_optimizer, vl_loss = value_grad\n",
    "    observation = env.reset()\n",
    "    totalreward = 0\n",
    "    states = []\n",
    "    actions = []\n",
    "    advantages = []\n",
    "    transitions = []\n",
    "    update_vals = []\n",
    "\n",
    "\n",
    "    for t in range(200):\n",
    "        # calculate policy\n",
    "        obs_vector = np.expand_dims(observation, axis=0)\n",
    "        probs = sess.run(pl_calculated,feed_dict={pl_state: obs_vector})\n",
    "        action = 0 if random.uniform(0,1) < probs[0][0] else 1\n",
    "        # record the transition\n",
    "        states.append(observation)\n",
    "        actionblank = np.zeros(2)\n",
    "        actionblank[action] = 1\n",
    "        actions.append(actionblank)\n",
    "        # take the action in the environment\n",
    "        old_observation = observation\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        transitions.append((old_observation, action, reward))\n",
    "        totalreward += reward\n",
    "        \n",
    "        if t % 3 == 0 and render:\n",
    "            env.render()\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "    for index, trans in enumerate(transitions):\n",
    "        obs, action, reward = trans\n",
    "\n",
    "        # calculate discounted monte-carlo return\n",
    "        future_reward = 0\n",
    "        future_transitions = len(transitions) - index\n",
    "        decrease = 1\n",
    "        for index2 in range(future_transitions):\n",
    "            future_reward += transitions[(index2) + index][2] * decrease\n",
    "            decrease = decrease * 0.97\n",
    "        obs_vector = np.expand_dims(obs, axis=0)\n",
    "        currentval = sess.run(vl_calculated,feed_dict={vl_state: obs_vector})[0][0]\n",
    "\n",
    "        # advantage: how much better was this action than normal\n",
    "        advantages.append(future_reward - currentval)\n",
    "\n",
    "        # update the value function towards new return\n",
    "        update_vals.append(future_reward)\n",
    "\n",
    "    # update value function\n",
    "    update_vals_vector = np.expand_dims(update_vals, axis=1)\n",
    "    sess.run(vl_optimizer, feed_dict={vl_state: states, vl_newvals: update_vals_vector})\n",
    "    # real_vl_loss = sess.run(vl_loss, feed_dict={vl_state: states, vl_newvals: update_vals_vector})\n",
    "\n",
    "    advantages_vector = np.expand_dims(advantages, axis=1)\n",
    "    sess.run(pl_optimizer, feed_dict={pl_state: states, pl_advantages: advantages_vector, pl_actions: actions})\n",
    "\n",
    "    return totalreward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_1qXVN0ACf2Z",
    "outputId": "f3725c8d-d3a1-4925-a3de-d381cf454514"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.0\n",
      "19.0\n",
      "18.0\n",
      "69.0\n",
      "200.0\n",
      "200.0\n",
      "200.0\n",
      "185.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-0c8629c044d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_episode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_grad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_grad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mt\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-ce90205b4db3>\u001b[0m in \u001b[0;36mrun_episode\u001b[0;34m(env, policy_grad, value_grad, sess, render)\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0mdecrease\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecrease\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.97\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0mobs_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0mcurrentval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvl_calculated\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mvl_state\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mobs_vector\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;31m# advantage: how much better was this action than normal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 950\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    951\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1156\u001b[0m     \u001b[0;31m# Create a fetch handler to take care of the structure of fetches.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1157\u001b[0m     fetch_handler = _FetchHandler(\n\u001b[0;32m-> 1158\u001b[0;31m         self._graph, fetches, feed_dict_tensor, feed_handles=feed_handles)\n\u001b[0m\u001b[1;32m   1159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1160\u001b[0m     \u001b[0;31m# Run request and get response.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, graph, fetches, feeds, feed_handles)\u001b[0m\n\u001b[1;32m    472\u001b[0m     \"\"\"\n\u001b[1;32m    473\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 474\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetch_mapper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_FetchMapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_fetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    475\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mget_controller\u001b[0;34m(self, default)\u001b[0m\n\u001b[1;32m   5654\u001b[0m       \u001b[0;31m# If an exception is raised here it may be hiding a related exception in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5655\u001b[0m       \u001b[0;31m# the try-block (just above).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5656\u001b[0;31m       \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext_switches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5657\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mpop\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0;34m\"\"\"Pop the stack.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "policy_grad = policy_gradient()\n",
    "value_grad = value_gradient()\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.initialize_all_variables())\n",
    "t = 0\n",
    "for i in range(2000):\n",
    "    reward = run_episode(env, policy_grad, value_grad, sess, render=True)\n",
    "    t += reward\n",
    "    if i % 100 == 0:\n",
    "        print(reward)\n",
    "\n",
    "print(t / 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "CS159_final.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
